"""
FINN.no Job Scraper Service
Scrapes tech job listings from FINN.no
"""

import httpx
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import logging
from urllib.parse import urlencode

from app.config import settings

logger = logging.getLogger(__name__)


class FINNScraper:
    """Scraper for FINN.no job listings"""

    BASE_URL = "https://www.finn.no/job/fulltime/search.html"

    def __init__(self):
        self.headers = {
            'User-Agent': settings.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    async def search_jobs(self, keyword: str, location: str = None, limit: int = 100) -> List[Dict]:
        """
        Search for jobs on FINN.no by keyword

        Args:
            keyword: Search keyword
            location: FINN location code (default from settings)
            limit: Maximum number of jobs to fetch

        Returns:
            List of job dictionaries
        """
        location = location or settings.finn_location
        jobs = []

        params = {
            'q': keyword,
            'location': location,
            'sort': 'PUBLISHED_DESC'
        }

        url = f"{self.BASE_URL}?{urlencode(params)}"
        logger.info(f"Scraping FINN.no for keyword: {keyword}")

        try:
            async with httpx.AsyncClient(timeout=settings.request_timeout) as client:
                response = await client.get(url, headers=self.headers)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')
                job_listings = soup.find_all('article', class_='sf-search-ad', limit=limit)

                for listing in job_listings:
                    job = self._parse_job_listing(listing, keyword)
                    if job:
                        jobs.append(job)

                logger.info(f"Found {len(jobs)} jobs for keyword '{keyword}'")

        except httpx.HTTPError as e:
            logger.error(f"HTTP error scraping FINN.no: {e}")
        except Exception as e:
            logger.error(f"Error scraping FINN.no: {e}")

        return jobs

    def _parse_job_listing(self, listing: BeautifulSoup, keyword: str) -> Optional[Dict]:
        """
        Parse a single job listing from FINN.no

        Args:
            listing: BeautifulSoup element containing job listing
            keyword: The keyword that found this job

        Returns:
            Dictionary with job data or None if parsing fails
        """
        try:
            # Extract title
            title_elem = listing.find('h2', class_='text-18') or listing.find('a', class_='sf-search-ad-link')
            title = title_elem.get_text(strip=True) if title_elem else None

            # Extract URL
            link_elem = listing.find('a', class_='sf-search-ad-link')
            url = f"https://www.finn.no{link_elem['href']}" if link_elem and 'href' in link_elem.attrs else None

            # Extract company
            company_elem = listing.find('div', class_='text-caption') or listing.find('span', class_='company-name')
            company = company_elem.get_text(strip=True) if company_elem else "Unknown"

            # Extract location
            location_elem = listing.find('div', class_='text-12') or listing.find('span', class_='location')
            location = location_elem.get_text(strip=True) if location_elem else None

            # Extract published date
            published_elem = listing.find('span', class_='sf-search-ad-published')
            published = published_elem.get_text(strip=True) if published_elem else None

            # Validate required fields
            if not title or not url:
                logger.warning("Skipping job listing - missing title or URL")
                return None

            return {
                'title': title,
                'company': company,
                'location': location,
                'url': url,
                'source': 'FINN',
                'keywords': keyword,
                'published': published,
                'deadline': None,  # FINN doesn't always show deadline in listings
                'job_type': None,   # Extract from detail page if needed
                'description': None,  # Need to fetch detail page
                'summary': None,      # Will be generated by AI service
                'scraped_date': datetime.now(),
                'status': 'ACTIVE',
            }

        except Exception as e:
            logger.error(f"Error parsing FINN job listing: {e}")
            return None

    async def fetch_job_details(self, url: str) -> Optional[Dict]:
        """
        Fetch full job details from a FINN.no job posting URL

        Args:
            url: FINN.no job posting URL

        Returns:
            Dictionary with detailed job information
        """
        try:
            async with httpx.AsyncClient(timeout=settings.request_timeout) as client:
                response = await client.get(url, headers=self.headers)
                response.raise_for_status()

                soup = BeautifulSoup(response.content, 'html.parser')

                # Extract description
                description_elem = soup.find('div', class_='import-decoration') or \
                                 soup.find('div', class_='u-word-break') or \
                                 soup.find('section', class_='panel')

                description = description_elem.get_text(strip=True) if description_elem else None

                # Extract deadline
                deadline_elem = soup.find('dt', string='SÃ¸knadsfrist')
                if deadline_elem:
                    deadline_value = deadline_elem.find_next_sibling('dd')
                    deadline = deadline_value.get_text(strip=True) if deadline_value else None
                else:
                    deadline = None

                # Extract job type
                job_type_elem = soup.find('dt', string='Stillingsfunksjon')
                if job_type_elem:
                    job_type_value = job_type_elem.find_next_sibling('dd')
                    job_type = job_type_value.get_text(strip=True) if job_type_value else None
                else:
                    job_type = None

                return {
                    'description': description,
                    'deadline': deadline,
                    'job_type': job_type,
                }

        except httpx.HTTPError as e:
            logger.error(f"HTTP error fetching FINN job details: {e}")
            return None
        except Exception as e:
            logger.error(f"Error fetching FINN job details: {e}")
            return None

    async def scrape_all_keywords(self, keywords: List[str] = None, limit_per_keyword: int = None) -> List[Dict]:
        """
        Scrape jobs for all keywords

        Args:
            keywords: List of keywords to search (default from settings)
            limit_per_keyword: Max jobs per keyword (default from settings)

        Returns:
            List of all job dictionaries
        """
        keywords = keywords or settings.get_keywords()
        limit_per_keyword = limit_per_keyword or settings.max_jobs_per_keyword

        # Apply max keywords limit if set
        if settings.max_keywords > 0:
            keywords = keywords[:settings.max_keywords]

        all_jobs = []

        for keyword in keywords:
            jobs = await self.search_jobs(keyword, limit=limit_per_keyword)
            all_jobs.extend(jobs)

            # Rate limiting - delay between requests
            if settings.request_delay > 0:
                import asyncio
                await asyncio.sleep(settings.request_delay)

        # Deduplicate by URL
        seen_urls = set()
        unique_jobs = []
        for job in all_jobs:
            if job['url'] not in seen_urls:
                seen_urls.add(job['url'])
                unique_jobs.append(job)

        logger.info(f"Scraped {len(unique_jobs)} unique jobs from {len(keywords)} keywords")
        return unique_jobs
