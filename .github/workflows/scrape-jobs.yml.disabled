name: Scrape Jobs

on:
  schedule:
    # Run twice daily: 7 AM and 5 PM UTC
    - cron: '0 7,17 * * *'

  # Allow manual triggering from Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run scraper
      env:
        # Database
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
        # Scraper Config (using secrets for all overrides)
        FINN_LOCATION: ${{ secrets.FINN_LOCATION }}
        NAV_COUNTY: ${{ secrets.NAV_COUNTY }}
        NAV_MUNICIPAL: ${{ secrets.NAV_MUNICIPAL }}
        MAX_JOBS_PER_KEYWORD: ${{ secrets.MAX_JOBS_PER_KEYWORD }}
        MAX_KEYWORDS: ${{ secrets.MAX_KEYWORDS }}
        KEYWORDS: ${{ secrets.KEYWORDS }} # Optional keyword override
        
        # Logging
        LOG_LEVEL: ${{ secrets.LOG_LEVEL }}
        
        # AI Config
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        ENABLE_SUMMARIZATION: ${{ secrets.ENABLE_SUMMARIZATION }}
        ENABLE_AI_FILTER: ${{ secrets.ENABLE_AI_FILTER }}
      run: python scraper.py

    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs
        path: |
          *.log
        retention-days: 7